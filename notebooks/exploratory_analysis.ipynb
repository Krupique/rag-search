{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rag-search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import PyPDF2\n",
    "from pptx import Presentation\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir):\n",
    "    # Initialize a empty list to store the file path\n",
    "    arquivo_list = []\n",
    "    \n",
    "    # Iterate all files and directories on specif directory\n",
    "    for f in listdir(dir):\n",
    "        \n",
    "        # If it's a file, add to the list\n",
    "        if isfile(join(dir, f)):\n",
    "            arquivo_list.append(join(dir, f))\n",
    "        \n",
    "        # If it is a directory, call the function recursively and add the results to the list\n",
    "        elif isdir(join(dir, f)):\n",
    "            arquivo_list += list_files(join(dir, f))\n",
    "    \n",
    "    # Return the list of files\n",
    "    return arquivo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the function that loads the text from a Word file \n",
    "def load_docx_text(arquivoname):\n",
    "    \n",
    "    # Open the Word file\n",
    "    doc = docx.Document(arquivoname)\n",
    "    \n",
    "    # Extract the text from each paragraph and add it to the list\n",
    "    fullText = [para.text for para in doc.paragraphs]\n",
    "    \n",
    "    \n",
    "    # Join all texts into a single string separated by line breaks\n",
    "    return '\\n'.join(fullText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that load the text from a PowerPoint file\n",
    "def load_pptx_text(arquivoname):\n",
    "    \n",
    "    # Open the PowerPoint file\n",
    "    prs = Presentation(arquivoname)\n",
    "    \n",
    "    # Initialize a empty list to store the texts\n",
    "    fullText = []\n",
    "    \n",
    "    # Iterate all slides\n",
    "    for slide in prs.slides:\n",
    "        \n",
    "        # Iterate all shapes on slides\n",
    "        for shape in slide.shapes:\n",
    "            \n",
    "            # If the shape has the \"text\" attributes, add it to the list \n",
    "            if hasattr(shape, \"text\"):\n",
    "                fullText.append(shape.text)\n",
    "    \n",
    "    # Join all texts into a single string separated by line breaks\n",
    "    return '\\n'.join(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the main function for indexing documents\n",
    "def main_indexing(mypath):\n",
    "    \n",
    "    # Sets the name of the model to be used to create the embeddings\n",
    "    model_name = \"sentence-transformers/msmarco-bert-base-dot-v5\"\n",
    "    model_kwargs = {'device': 'cpu'} # Model settings\n",
    "    encode_kwargs = {'normalize_embeddings': True} # Set encoding settings\n",
    "\n",
    "    # Initialize the HuggingFace embeddings class\n",
    "    hf = HuggingFaceEmbeddings(model_name = model_name,\n",
    "                               model_kwargs = model_kwargs,\n",
    "                               encode_kwargs = encode_kwargs)\n",
    "\n",
    "    client = QdrantClient(\"http://localhost:6333\") # Initialize the Qdrant client\n",
    "    collection_name = \"VectorDB\" # Sets the name of the embeddings collection\n",
    "\n",
    "    if client.collection_exists(collection_name): # If the collection already exists, delete it\n",
    "        client.delete_collection(collection_name)\n",
    "\n",
    "    \n",
    "    client.create_collection(collection_name, # Creates a new collection with specified parameters\n",
    "                             vectors_config = VectorParams(size = 768, distance = Distance.DOT))\n",
    "\n",
    "    qdrant = Qdrant(client, collection_name, hf) # Initialize the Qdrant instance\n",
    "\n",
    "    #Prints a message informing that document indexing is starting\n",
    "    print(\"\\nIndexing the documents...\\n\")\n",
    "\n",
    "    files_list = list_files(mypath)\n",
    "    \n",
    "    for file in files_list:\n",
    "        \n",
    "        try:\n",
    "            # Initialize a empty string to store the file content\n",
    "            file_content = \"\"\n",
    "            \n",
    "            if file_content.endswith(\".pdf\"):\n",
    "                print(\"Indexing: \" + file_content)\n",
    "                reader = PyPDF2.PdfReader(file_content)\n",
    "                for page in reader.pages:\n",
    "                    file_content += \" \" + page.extract_text()\n",
    "            \n",
    "            elif file.endswith(\".txt\"):\n",
    "                print(\"Indexing: \" + file)\n",
    "                with open(file, 'r') as f:\n",
    "                    file_content = f.read()\n",
    "            \n",
    "            elif file.endswith(\".docx\"):\n",
    "                print(\"Indexing: \" + file)\n",
    "                file_content = load_docx_text(file)\n",
    "            \n",
    "            elif file.endswith(\".pptx\"):\n",
    "                print(\"Indexing: \" + file)\n",
    "                file_content = load_pptx_text(file)\n",
    "            \n",
    "            else:\n",
    "                # If the file isn't of a supported format, continue to the next one.\n",
    "                continue\n",
    "\n",
    "            # Inicializa o divisor de texto com tamanho de chunk e sobreposição especificados\n",
    "            text_splitter = TokenTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "            \n",
    "            # Spliting the file content in text chunks\n",
    "            textos = text_splitter.split_text(file_content)\n",
    "            \n",
    "            # Create metadata for each text chunk\n",
    "            # It allows that the LLM quotes the reference\n",
    "            metadata = [{\"path\": file} for _ in textos]\n",
    "            \n",
    "            # Add the texts and its metadatas to Qdrant\n",
    "            qdrant.add_texts(textos, metadatas = metadata)\n",
    "\n",
    "        except Exception as e:\n",
    "            # If an error occurs, print an error message\n",
    "            print(f\"The process failed for the file {file}: {e}\")\n",
    "\n",
    "    # Prints message informing that indexing is complete\n",
    "    print(\"\\nIndexing Completed!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
